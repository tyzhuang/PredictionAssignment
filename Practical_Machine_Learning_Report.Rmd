---
title: Predicting Manner of Exercising from Accelerometer Measurements Using Machine
  Learning
author: "Tan Yeong Zhuang(25 Oct 2017)"
output: html_document
---

## Background
This project aims to predict in which manner participants completed a single exercise, based on recordings provided by accelerometers attached to 4 measurement points on the body. Six male participants aged 20-28 were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in one correct, and 4 incorrect fashions. Specifically, the exercises were performed:  
* Exactly according to the specification (__Class A__);  
* Throwing elbows to the front (__Class B__);  
* Lifting the dumbbell only halfway (__Class C__);  
* Lowering the dumbbell only halfway (__Class D__); and  
* Throwing the hips to the front (__Class E__).

## Loading in the data and splitting into training and testing sets

The labelled dataset was [downloaded](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) on September 22nd, 2015 and loaded into R.

```{r labelled_data_loading, message = FALSE, cache = TRUE}
rm(list = ls())
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "data.csv")
data <- read.csv("data.csv")
```

There were a total of `r nrow(data)` observations in the labelled dataset, which was large enough to split into training and testing datasets. Sixty percent of the data were assigned to the training set and 40% to the testing set.

```{r data_splitting, message = FALSE, cache = TRUE}
require(caret)
set.seed(567)
inTrain <- createDataPartition(y = data$classe, p = 0.6, list = FALSE)
training <- data[inTrain, ]
testing <- data[-inTrain, ]
```

There were a total of `r nrow(training)` observations in the training set and `r nrow(testing)` in the testing set. The unlabelled validation dataset was also [downloaded](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv).

```{r unlabelled_data_training, message = FALSE, cache = TRUE}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "validation.csv")
validation <- read.csv("validation.csv")
rm(list = c("data", "inTrain"))
```

The validation dataset had `r nrow(validation)` observations.

## Data screening and covariate inspection (training set)

The data were initially inspected. A number of variables appeared to have missing variables (e.g., `max_roll_belt`), and also variables that appeared numeric were classed as factor variables (e.g., `kurtosis_roll_belt`. In addition to the `r ncol(training[8:159])` predictor variables and the outcome, there also appeared to be a number of variables describing the time in which the exercise was performed and which of the participants performed the exercise.

```{r data_inspection, message = FALSE, warning = FALSE}
str(training)
```

### Outcome variable

The outcome variable was fairly evenly distributed between the 5 categories, with the smallest class (Class D) containing `r min(table(training$classe))` observations (`r round(min(prop.table(table(training$classe))) * 100)`%), and the largest class (Class A) containing `r max(table(training$classe))` observations (`r round(max(prop.table(table(training$classe))) * 100)`%).

```{r frequency_outcome}
table(training$classe)
prop.table(table(training$classe))
```

### Cleaning factor variables

The predictors were cleaned so that those that were incorrectly classified as factor were converted to numeric.

```{r factor_to_numeric, message = FALSE, warning = FALSE}
require(magrittr)

asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)], 
                                                asNumeric))
training %>%
    subset(select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, 
                      cvtd_timestamp, new_window, num_window, classe)) %>%
    factorsNumeric -> training[ , 8:159]
```

### Screening for missing variables

The variables were then inspected for missingness.

```{r missingness_screen, message = FALSE, warning = FALSE}
propmiss <- function(dataframe) {
    sapply(dataframe,function(x) 
    data.frame(nmiss=sum(is.na(x)), 
               n=length(x), 
               propmiss=sum(is.na(x))/length(x)))
}

propmiss(training)
n.missing <- sum(sapply(training, function(x) sum(is.na(x))/length(x) >= 0.05))
training <- training[, sapply(training, function(x) sum(is.na(x))/length(x) < 0.05)]
```

A large number of predictors (`r n.missing`) have over 90% of the values missing. Due to the high number with missing values, as well as the very high percentage missingness, it was decided that replacement techniques would be inappropriate and these variables were excluded from the dataset. This left a list of 52 predictors.

```{r variables_left, message = FALSE, warning = FALSE}
require(knitr)

vars_left <- data.frame(Device <- character(length = 52), 
                        Predictor <- colnames(training[8:59]))
names(vars_left) <- c("Device", "Predictor")
vars_left$Device <- as.character(vars_left$Device)
vars_left$Predictor <- as.character(vars_left$Predictor)

vars_left$Device[grepl("belt", vars_left$Predictor, ignore.case = TRUE)] <- "Belt"
vars_left$Device[grepl("arm", vars_left$Predictor, ignore.case = TRUE)] <- "Arm"
vars_left$Device[grepl("dumbbell", vars_left$Predictor, ignore.case = TRUE)] <- "Dumbbell"
vars_left$Device[grepl("forearm", vars_left$Predictor, ignore.case = TRUE)] <- "Forearm"

vars_left$Predictor <- gsub("_belt|_arm|_dumbbell|_forearm", "", vars_left$Predictor)
kable(table(vars_left$Predictor, vars_left$Device))
```

**Table 1.** Contingency table of predictors which will be used for the machine learning algorithms to predict how unilateral dumbbell biceps curl is performed.

As demonstrated by Table 1, each of the 4 measurement points had one each of the represented measures.

### Screening for near-zero variance variables

```{r near_zero_variance_screening, message = FALSE}
require(caret)

nzv <- nearZeroVar(training[8:59], saveMetrics = TRUE)
nzv
```

None of the 52 predictors have zero- or near-zero variance, meaning all can be retained on this basis.

### Correlations between variables

Collinear variables (those with correlations at or above 0.8) were screened for.

```{r predictor_intercorrelations}
spec.cor <- function (dat, r, ...) { 
    x <- cor(dat, ...) 
    x[upper.tri(x, TRUE)] <- NA 
    i <- which(abs(x) >= r, arr.ind = TRUE) 
    data.frame(matrix(colnames(x)[as.vector(i)], ncol = 2), value = x[i]) 
} 

spec.cor(training[ , 8:59], 0.8)
```

A number of the variables are collinear, which may introduce some noise into the models if collinear pairs are retained. If neither of the models perform well in cross-validation, trimming on the basis of these associations could be considered to improve model fit.

## Cross-validation and out-of-sample error estimation

Two possible algorithms were considered to predict how the biceps curl was performed: decision tree and random forests classification. Each of these models were built using all 52 predictors. K-fold cross validation with 10 folds was used for each. The out-of-sample error was estimated by:  
* Fitting the model on the "training" proportion of each fold;  
* Predicting the outcome on the "testing" proportion of each fold;  
* Generating the accuracy measure for each fold by constructing a confusion matrix between the predicted and actual outcome on the testing proportion; and  
* Taking the mean and standard deviation of the 10 accuracy scores from each fold.

### Decision tree classification

```{r decision_tree_cv, message = FALSE, cache = TRUE}
set.seed(567)
folds <- createFolds(training$classe, k = 10, list = TRUE, returnTrain = TRUE)
accuracies.dt <- c()
for (i in 1:10) {
    model <- train(classe ~ ., data=training[folds[[i]], 8:60], method = "rpart")
    predictions <- predict(model, training[-folds[[i]],8:59])
    accuracies.dt <- c(accuracies.dt, 
                       confusionMatrix(predictions, training[-folds[[i]], 8:60]$classe)$overall[[1]])
}
```

The mean accuracy of the decision tree classifications was `r round(mean(accuracies.dt), 3)` and the standard deviation was `r round(sd(accuracies.dt), 3)`. This projected out-of-sample error is unacceptably high, indicating that decision tree classification is not appropriate to predict this outcome.

### Random forests classification

```{r random_forests_cv, message = FALSE, cache = TRUE}
set.seed(567)
accuracies.rf <- c()
for (i in 1:10) {
    model <- train(classe ~ ., data=training[folds[[i]], 8:60], method = "rf")
    predictions <- predict(model, training[-folds[[i]],8:59])
    accuracies.rf <- c(accuracies.rf, 
                       confusionMatrix(predictions, training[-folds[[i]], 8:60]$classe)$overall[[1]])
}
```

The mean accuracy of the random forests classification was `r round(mean(accuracies.rf), 3)` and the standard deviation was `r round(sd(accuracies.rf), 3)`. This projected out-of-sample error is very low, indicating that random forests classification is likely to be appropriate to predict this outcome.

## Predicting on testing data

### Data screening and cleaning

The distribution of the outcome was first double-checked in the testing set:

```{r frequency_outcome_testing}
table(testing$classe)
prop.table(table(testing$classe))
```

The distribution was similar to the training set (as expected), with a good number of observations in each category.

The transformation applied to the testing set (factor variables converted to numeric) was applied to the testing set.

```{r factor_to_numeric_testing, message = FALSE, warning = FALSE}
require(magrittr)

asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)], 
                                                asNumeric))
testing %>%
    subset(select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, 
                      cvtd_timestamp, new_window, num_window, classe)) %>%
    factorsNumeric -> testing[ , 8:159]
```

### Predicting the outcome on the test set

```{r predict_on_test_set, warnings = FALSE, cache = TRUE}
# Build model on whole training set
set.seed(567)
model <- train(classe ~ ., data = training[ , 8:60], method = "rf")
# Make predictions on testing set
predictions <- predict(model, testing[,8:159])
# Summarise results
confusionMatrix(predictions, testing$classe)
```

### Evaluating the model

The importance of the variables used in the model were assessed using an indication of variable importance. The 20 most important variables are listed below:

```{r variable_importance, message = FALSE}
varImp(model)
```

The top three predictors are modelled below. As you can see, their densities per outcome class are very similar between the training and testing sets, meaning they have good discriminant abilities that are not tied to the training set.

```{r density_plots, message = FALSE, warning = FALSE, fig.height = 14, fig.width = 14}
d.plots <- function(predictor, data, title, label) {
    qplot(predictor, colour = classe, data = data, geom = "density") + 
        xlab(label) +
        ylab("Density") +
        ggtitle(title) +
        theme_bw()
}

g1 <- d.plots(training$roll_belt, training, "Belt Roll (Training)", "Belt roll")
g2 <- d.plots(testing$roll_belt, testing, "Belt Roll (Testing)", "Belt roll")
g3 <- d.plots(training$pitch_forearm, training, "Forearm Pitch (Training)", "Forearm pitch")
g4 <- d.plots(testing$pitch_forearm, testing, "Forearm Pitch (Testing)", "Forearm pitch")
g5 <- d.plots(training$yaw_belt, training, "Belt Yaw (Training)", "Belt yaw")
g6 <- d.plots(testing$yaw_belt, testing, "Belt Yaw (Testing)", "Belt yaw")

require(gridExtra)
grid.arrange(g1, g2, g3, g4, g5, g6, nrow = 3, ncol = 2)
```

**Figure 2.** Three most important predictors and their distributions by outcome class in the training and testing sets.

These top three predictors indicate that Class E has higher belt roll than the other classes, Class A has lower forearm pitch and Class D has higher forearm pitch than the other classes, and Class E has higher belt yaw.

## Predicting the unlabelled validation set

Finally, the outcome was predicted using the random forests algorithm on the unlabelled data. The results are displayed below.

```{r factor_to_numeric_validation, message = FALSE, warning = FALSE}
require(magrittr)

asNumeric <- function(x) as.numeric(as.character(x))
factorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)], 
                                                asNumeric))
validation %>%
    subset(select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, 
                      cvtd_timestamp, new_window, num_window, problem_id)) %>%
    factorsNumeric -> validation[ , 8:159]
```

```{r predict_on_validation_set, warnings = FALSE}
# Make predictions on validation set
predictions <- predict(model, validation[,8:159])
# Print predictions
data.frame(problem_id = validation$problem_id, prediction = predictions)
```


